# Core requirements for Samoey Copilot - FREE EDITION
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dotenv==1.0.0
pydantic==2.4.2
pydantic-settings==2.0.3

# Database
sqlalchemy==2.0.23
alembic==1.12.1
psycopg2-binary==2.9.9

# AI/ML - FREE LOCAL MODELS ONLY
numpy==1.26.0
pandas==2.1.1

# AI/ML - Local Model Management (FREE)
ollama==0.1.0  # Local model management
llama-cpp-python==0.2.0  # Local LLM inference
ctransformers==0.2.0  # Fast transformers
torch==2.1.0
torchvision==0.16.0
transformers==4.35.2
sentence-transformers==2.2.2
langchain==0.0.300
chromadb==0.4.15
huggingface-hub==0.18.0
accelerate==0.24.1
bitsandbytes==0.41.1
datasets==2.14.5
tokenizers==0.15.0

# AI/ML - Vector & Embedding (FREE)
faiss-cpu==1.7.4
scikit-learn==1.3.2
nltk==3.8.1
spacy==3.7.2

# AI/ML - Text Processing (FREE)
tiktoken==0.5.1
beautifulsoup4==4.12.2
requests==2.31.0
aiofiles==23.2.1

# Async
anyio==3.7.1
httpx==0.25.0
aiohttp==3.9.0

# Security
cryptography==41.0.4
bcrypt==4.0.1
python-jose[cryptography]==3.3.0
secrets==1.0.0  # For auto-generating keys

# Utils
python-dateutil==2.8.2
email-validator==2.1.0.post1
prometheus-client==0.19.0
psutil==7.0.0
pydantic[email]==2.4.2

# Caching
redis==5.0.1
redis-om==0.3.2

# Background Tasks
celery==5.3.4
celery[redis]==5.3.4

# Monitoring and Logging
sentry-sdk==1.31.0
python-json-logger==2.0.7
structlog==23.2.0

# Development & Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
black==23.11.0
flake8==6.1.0
mypy==1.7.1

# API Documentation
redoc==2.0.0
markdown2==2.4.9

# File Processing
Pillow==10.1.0
python-magic==0.4.27

# Rate Limiting
slowapi==0.1.8
limits==3.5.0

# WebSocket
websockets==12.0
socketio==5.9.0

# Environment Management
python-dotenv==1.0.0
click==8.1.7
rich==13.7.0

# Local Model Serving
vllm==0.2.0  # Fast LLM serving
text-generation-webui==0.0.0  # Local web UI for models

# Additional Free AI Libraries
auto-gptq==0.4.2  # Quantized models
exllama==0.0.8  # Optimized LLaMA inference
gpt4all==2.0.0  # Local quantized models
